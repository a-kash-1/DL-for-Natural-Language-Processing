{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akash.sharma\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Steps to be followed: \n",
    "\n",
    "1. Load dependencies\n",
    "2. Prepare corpus\n",
    "3. Define Model\n",
    "4. Analyze Model\n",
    "5. Plot word cluster using t-SNE\n",
    "6. Plot model on tensorboard\n",
    "'''\n",
    "\n",
    "import multiprocessing\n",
    "import os, json, requests\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash.sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akash.sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def _to_wordlist(raw):\n",
    "    \n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",raw)\n",
    "    words = clean.split()\n",
    "    return (list(map(lambda x:x.lower(), words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"http://www.gutenberg.org/files/33224/33224-0.txt\"\n",
    "raw_corpus = requests.get(filepath)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#from nltk import sent_tokenize\n",
    "raw_sentences = tokenizer.tokenize(raw_corpus.text)\n",
    "#raw_sentences = sent_tokenize(raw_corpus.text)\n",
    "sentences = []\n",
    "\n",
    "for sentence in raw_sentences:\n",
    "    if(len(raw_sentences) > 0):\n",
    "        sentences.append(_to_wordlist(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35990527, 51075960)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining gensim model\n",
    "\n",
    "#Arguments being seed- for generating same output later, workers = no. of cpu threads, min_count = min. frequency of words, window = context size, sample = downsampling\n",
    "model = w2v.Word2Vec(sg = 1, seed = 1, workers = multiprocessing.cpu_count(), min_count = 3, window = 7, sample = 1e-3)\\\n",
    "\n",
    "#So building vocab first\n",
    "model.build_vocab(sentences)\n",
    "#Training the model\n",
    "model.train(sentences, total_examples = model.corpus_count, epochs =120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(\"trained\", 'sample')):\n",
    "    os.makedirs(os.path.join(\"trained\", 'sample'))\n",
    "    \n",
    "model.save(os.path.join(\"trained\", 'sample',\".w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akash.sharma\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('serene', 0.8594950437545776),\n",
       " ('predominates', 0.6454615592956543),\n",
       " ('luxuriantly', 0.6405662894248962),\n",
       " ('tempestuous', 0.5351181030273438),\n",
       " ('feeble', 0.50838702917099),\n",
       " ('weather', 0.5022791624069214),\n",
       " ('whatsoever', 0.4882858097553253),\n",
       " ('monsoons', 0.47658997774124146),\n",
       " ('intercepted', 0.4733385145664215),\n",
       " ('delightful', 0.4719574451446533)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding relations between the words using most_similar()\n",
    "model.wv.most_similar('sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'laughter' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-40125be799e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#When we call most_similar on a word not in the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Limitation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'laughter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\akash.sharma\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\akash.sharma\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'laughter' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#When we call most_similar on a word not in the vocabulary\n",
    "#Limitation \n",
    "model.wv.most_similar('laughter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Analogy\n",
    "model.most_similar_cosmul(positive =['earth', 'moon'], negative =['orbit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using t-SNE algorithm (used for dimensionality reduction)\n",
    "tane = sklearn.manifold.TSNE(n_components=2, random_state =0)\n",
    "all_word_vectors_matrix = model.wv.vectors\n",
    "all_word_vectors_matrix_2d = tane.fit_transform(all_word_vectors_matrix)\n",
    "\n",
    "points = pd.DataFrame([ (word, coords[0], coords[1]) for word, coords in [ (word, all_word_vectors_matrix_2d[model.wv.vocab[word].index]) for word in model.wv.vocab]], columns= [\"word\", \"x\", \"y\"])\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "ax = points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))\n",
    "fig = ax.get_figure()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = points.word.values.tolist()\n",
    "embeddings = all_word_vectors_matrix\n",
    "\n",
    "embedding_var = tf.Variable(all_word_vectors_matrix, dtype= 'float32', name = 'embedding')\n",
    "projector_config = projector.ProjectorConfig()\n",
    "\n",
    "embedding = projector_config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "LOG_DIR = './'\n",
    "metadata_file = os.path.join(\"sample.tsv\")\n",
    "\n",
    "with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata: \n",
    "    metadata.writelines(\"%s\\n\" % w.encode('utf=8') for w in vocab_list)\n",
    "    \n",
    "embedding.metadata_path = os.path.join(os.getcwd(), metadata_file)\n",
    "\n",
    "#Saving summary in LOG_DIR\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "#The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "#read this file during startup \n",
    "\n",
    "projector.visualize_embeddings(summary_writer, projector_config)\n",
    "\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Initialize the model\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.save(sess, os.path.join(LOG_DIR, metadata_file + '.ckpt'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-1-7e419fd58fcb>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7e419fd58fcb>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, 12_reg_lambda = 0, pre_trained =False):\u001b[0m\n\u001b[1;37m                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "# Building a sentiment classifier using CNN\n",
    "\n",
    "class TextCNN(object):\n",
    "    '''Embedding layer -> Convolutional -> Max-Pooling -> Softmax'''\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, 12_reg_lambda = 0, pre_trained =False):\n",
    "        \n",
    "        #Placeholders\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, numm_classes], name = \"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_probs\")\n",
    "        #Keeping track of regularization losses\n",
    "        12_loss = tf.constant(0.0)\n",
    "        \n",
    "        #Embedding Layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            if pre_trained:\n",
    "                W_ = tf.variable(tf.constant(0, shape = [vocab_size, embedding_size]), trainable = False, name ='W')\n",
    "                self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size], name = 'pre-trained')\n",
    "                W= tf.assign(W_, self.embedding_placeholder)\n",
    "                else:\n",
    "                    W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, name =\"W\"))\n",
    "                    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "        \n",
    "        #Convolution  + pooling\n",
    "        \n",
    "        pooled_output = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                #Convolution layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name = \"W\")\n",
    "                b = tf.variable(tf.constant(0.1, shape = [num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded, W, strides =[1,1,1,1], padding ='VALID', name =\"pool\") \n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        #Combining pooled features\n",
    "        num_filters_total = num_filters*len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        #Dropout for regularization from overfitting\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_deop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        #Predictions by tf.argmax()\n",
    "        \n",
    "        #Final scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape =[num_filters_total, num_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape = [num_classes]), name =\"b\")\n",
    "            l2_loss += tf.nn.12_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name = \"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name = \"predictions\")\n",
    "            \n",
    "        \n",
    "        #Evaluation \n",
    "        \n",
    "        #Mean-Entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits = self.scores)\n",
    "            self.loss = tf.reduce_mean(losses) + 12_reg_lambda*12_loss\n",
    "            \n",
    "        #Accuracy\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name =\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
